% When using TeXShop on the Mac, let it know the root document. The following must be one of the first 20 lines.
% !TEX root = ../design.tex

\chapter[Generalized Linear Models]{Generalized Linear Models}

\begin{moduleinfo}
\item[Author] \href{mailto:lpei@gopivotal.com}{Liquan Pei}
\item[History]
    \begin{modulehistory}
        \item[v0.1] Initial version
    \end{modulehistory}
\end{moduleinfo}

% Abstract. What is the problem we want to solve?

\section{Introduction}

Linear regression model assumes that the dependent variable $Y$ is equal to a linear combination $\vec{X}^\top \vec{\beta}$ and a normally distributed error term
\begin{align*}
Y = \vec{X}^\top \vec{\beta} + \epsilon
\end{align*}
where $\vec{\beta} = (\beta_{1}, \dots, \beta_{m})^\top$ is a vector of unknown parameters and $\vec{X} = (X_1, \dots, X_{m})^\top$ is a vector of independent variables.

In a generalized linear model (GLM), the distribution of dependent variable $Y$ is a member from the \emph{exponential family} and the mean $\mu = \mathbf{E}(Y)$ depends on the independent variables $\vec{X}$ through
\begin{align*}
\mu = \mathbf{E}(Y) = g^{-1}(\eta) =g^{-1}(\vec{X}^\top \beta)
\end{align*}
where $\eta = \vec{X}^\top \beta$ is the \emph{linear predictor} and $g$ is the \emph{link function}.

In what follows, we denote $G(\eta) = g^{-1}(\eta)$ as the inverse link function.

\subsection{Exponential Family}
A random variable $Y$ is a member from the exponantial family if its probability function or its density function has the form
\begin{align*}
f(y,\theta, \psi) = \exp \left\{ \frac{y\theta - b(\theta)}{a(\psi)} + c(y, \psi)\right\}
\end{align*}
where $\theta$ is the canonical parameter.
The mean and variance of the exponential family are
\begin{itemize}
\item $\mathbf{E}(Y) = \mu = b'(\theta)$
\item $\mathbf{Var}(Y) = V(\mu)a(\psi) = b''(\theta)a(\psi)$
\end{itemize}

\subsection{Linear Predictor}
The linear predictor $\eta$ incorporates the information about the independent variables into the model. It is related to the expected value of the data through link functions.

\subsection{Link Function}
The link function provides the relationship between $\eta$, the linear predictor and $\mu$, the mean of the distribution function. There are many commonly used link functions, and their choice can be somewhat arbitrary. It makes sense to try to match the domain of the link function to the range of the distribution function's mean.

For canonical parameter $\theta$, the canonical link function is the function that expresses $\theta$ in terms of $\eta = g(\mu)$. In what follows we treat $\theta = \theta(\eta) = h(g(\mu))$. If we choose $h$ to be an identical function, then $\theta = \eta$
and $\mu = G(\eta) = \mu(\theta)$.

\section{Parameter Estimation}
We estimate unknown parameters $\beta$ by maximizing the log-likelihood of a GLM. Given the examples $\vec{Y} = (Y_1, \dots, Y_n)^\top$ and denote their mean as $\vec{\mu} = (\mu_1, \dots, \mu_{n})^\top$, the log-likelihood for a GLM is

\begin{align*}
l(\vec{Y}, \vec{\mu}, \psi) & =\sum_{i=1}^{n} \log f(Y_i, \theta_{i}, \psi) \\
& =\sum_{i=1}^{n}\left\{ \frac{Y_i \theta_{i} - b\left(\theta_{i}\right)}{a\left(\psi\right)} - c(Y_i, \psi)\right\}
\end{align*}
where $\theta_{i} = \theta(\eta_{i}) = \theta(x_i^\top \beta)$

Note that $a(\psi)$ and $c(Y_i, \psi)$ dose not depend on $\beta$, we then maximize
\begin{align}
\label{likelihood}
\tilde{l}(\vec{Y}, \vec{\mu}) = \sum_{i = 1}^{n} \left\{ Y_i \theta_{i} - b(\theta_{i}) \right\}
\end{align}
with respect to $\vec{\beta}$.
In what follows, we denote $\vec{x_i}$ to be the vector of values of independent variables for $Y_i$.
\subsection{Iterative Reweighted Least Squares Algorithm}
We use iterative reweighted least squares (IRLS) algorithm to find $\vec{\beta}$ that maximize $\tilde{l}(\vec{Y},\vec{\mu})$. Specifically, we use Fisher scoring algorithm which updates $\vec{\beta}$ at step $k+1$ using
\begin{align*}
\vec{\beta}^{k+1} = \vec{\beta}^{k} + \left\{\mathbf{E}[H(\beta^{k})]\right\}^{-1} \nabla_{\vec{\beta}}\tilde{l}(\vec{\beta}^{k})
\end{align*}
where $\mathbf{E}[H]$ is the mean of Hessian over examples $\vec{Y}$ and $\nabla_{\vec{\beta}}\tilde{l}$ is the gradient.
For GLM, the gradient is
\begin{align*}
\nabla_{\vec{\beta}}\tilde{l} &= \sum_{i = 1}^{n}\left\{Y_i - b'(\theta_{i})\right\}\nabla_{\vec{\beta}}\theta_{i} \\
\end{align*}
Note that $\mu_{i} = G(\eta_{i}) = G(\vec{x_i}^\top \vec{\beta}) = b'(\theta_{i})$, we have
\begin{align*}
\nabla_{\vec{\beta}}\theta_{i} &= \frac{G'(\eta_{i})}{V(\mu_{i})} \vec{x_i}
\end{align*}
%and
%\begin{align*}
%\frac{\partial^2 \theta_{i}}{\partial \beta \partial \beta^{T}} = \frac{G''(\eta_{i})V(\mu_{i}) - G'(\eta_{i})^2 V'(\mu_i)}{V(\mu_{i})^2}x_i x_i^T
%\end{align*}
then
\begin{align*}
\nabla_{\vec{\beta}}\tilde{l} &= \sum_{i = 1}^{n}\left\{Y_i - \mu_{i}\right\}\frac{G'(\eta_{i})}{V(\mu_{i})}\vec{x_i}
\end{align*}
The Hessian is
\begin{align*}
H(\beta) &= \sum_{i = 1}^{n} \left\{-b''(\theta_{i}) \nabla_{\vec{\beta}}\theta_{i} {\nabla_{\vec{\beta}}\theta_{i}}^\top - \left\{Y_i - b'(\theta_{i})\right\}\nabla_{\vec{\beta}}^2 \theta_{i}\right\} \\
& = \sum_{i = 1}^{n} \left\{ \frac{G'(\eta_{i})^2}{V(\mu_{i})} - \left\{Y_i - \mu_{i}\right\}\nabla_{\vec{\beta}}^2 \theta_{i}\right\}\vec{x_i} \vec{x_i}^\top
\end{align*}
Note that $\mathbf{E}[Y_i] = \mu_{i}$, we have
\begin{align*}
\mathbf{E}[H(\beta)] = \sum_{i = 1}^{n} \left\{\frac{G'(\eta_{i})^2}{V(\mu_{i})}\right\}\vec{x_i} \vec{x_i}^\top
\end{align*}
Define the weight matrix
\begin{align*}
\vec{W} = \text{diag}\left( \frac{G'(\eta_{1})^2}{V(\mu_{1})}, \dots, \frac{G'(\eta_{n})^2}{V(\mu_{n})} \right)
\end{align*}
and define
\begin{align*}
\vec{\tilde{Y}} = \left(\frac{Y_1 - \mu_1}{G'(\eta_{1})}, \dots, \frac{Y_n - \mu_{n}}{G'(\eta_{n})}\right)^\top
\end{align*}
and the design matrix
\begin{align*}
\vec{X}^\top = \left(\vec{x_1}, \dots, \vec{x_n}\right)
\end{align*}
Finally, the update rule for GLM is
\begin{align*}
\vec{\beta}^{k+1}  &= \vec{\beta}^{k} + (\vec{X}^\top \vec{W} \vec{X})^{-1}\vec{X}^\top \vec{W}\tilde{\vec{Y}} \\
&= (\vec{X}^\top \vec{W} \vec{X})^{-1}\vec{X^\top WZ}
\end{align*}
where  $\vec{Z} = (Z_1, \dots, Z_n)$ is a vector of \emph{adjusted dependent variables}
\begin{align*}
Z_i = \vec{x_i}^\top \vec{\beta}^{k} + \frac{Y_i - \mu_{i}}{G'(\eta_{i})}
\end{align*}
Note that each step is the result of a weighted least square regression on the adjusted variables $Z_i$ on $x_i$ and this the reason that this algorithm is called iterative reweighted least squares.

The IRLS algorithm for GLM is as follows
\begin{algorithm}
\alginput{$\vec{X}$, $\vec{Y}$, inverse link function $G(\eta)$, dispersion function $V(\mu)$ and initial values $\vec{\beta}^0$}
\algoutput{$\vec{\beta}$ that maximize $\tilde{l}(\vec{Y}, \vec{\mu})$}
\begin{algorithmic}[1]
    \State $k \leftarrow 0$
    \Repeat
        \State Compute $\vec{\mu}$ where $\mu_{i} = G(\eta_{i}) = G(\vec{x_i}^\top \vec{\beta}^{k})$
        \State Compute $\vec{Z}$ where $Z_i = \vec{x_i}^\top \vec{\beta}^{k} + \frac{Y_i - \mu_{i}}{G'(\eta_{i})}$
        \State Compute $\vec{W}$ where $W_{ii} = \frac{G'(\eta_{i})^2}{V(\mu_{i})}$
        \State $\vec{\beta}^{k+1} = \vec{(X^\top W X)}^{-1} \vec{X^\top WZ}$
    \Until{$\vec{\beta}^{k+1}$ converges}
\end{algorithmic}
\label{alg:IRLS}
\end{algorithm}

\subsection{Functions for contructing the exponential families}
Table \ref{tab:glm_func} \cite{fox2008applied} provides functions $a(), b()$ and $c()$ to contruction the exponential families,

\begin{table}[h]
\centering
\begin{tabular}{cccc}
Family & $a(\psi)$ & $b(\theta)$ & $\c(y, \psi)$ \\
\hline
Gaussian & $\psi$ & $\theta^2/2$ & $-\frac{1}{2}\left[y^2/\psi+\log_e(2\pi\psi)\right]$ \\
Binomial & 1/n & $\log_e(1+e^\theta)$ & $\log_eC^n_{ny}$ \\
Poisson & 1 & $e^\theta$ & $-log_ey!$ \\
Gamma & $\psi$ & $-\log_e(-\theta)$ & $\psi^{-1}\log_e(y/\psi) - \log_ey - \log_e\Gamma(\psi^{-1})$ \\
Inverse-Gaussian & $-\psi$ & $\sqrt{2\theta}$ & $-\frac{1}{2}\left[\log_e(\pi\psi y^3)^3 + 1/(\psi y) \right]$ \\
\end{tabular}
\caption{Functions for constructing the exponential families}
\label{tab:glm_func}
\end{table}

\subsection{Implementation}
Some implementation tips.

